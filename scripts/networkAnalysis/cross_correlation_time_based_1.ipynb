{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"09xiXPMziJhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import pywt\n","\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import folium\n","import geopandas as gpd\n","\n","import itertools\n","\n","# Get the current working directory + desired directory\n","parent_directory = os.getcwd()\n","main_directory = parent_directory + '/drive/MyDrive/2024 UW ENGINE Capstone'\n","curr_directory = main_directory + '/data/GMD and GIC'"],"metadata":{"id":"Kcu93-joiO0c"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRmN6JD9iC_z"},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import pywt\n","import itertools\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","\n","eventDate = 20131002 # YYYYMMDD\n","\n","# folder_path = 'C:\\\\Users\\\\19606\\\\Desktop\\\\GMDandGIC\\\\GICdata\\\\event_20131002\\\\GIC\\\\'\n","\n","parent_directory = os.getcwd()\n","main_directory = parent_directory + '/drive/MyDrive/2024 UW ENGINE Capstone'\n","curr_directory = main_directory + '/data/GMD and GIC'\n","event_directory = curr_directory + '/event_' + str(eventDate) + '/GIC'\n","\n","folder_path = event_directory\n","\n","time_column = 'SampleDateTime'\n","gic_column = 'GICMeasured'\n","\n","file_names = [f for f in os.listdir(event_directory) if f.startswith(str(eventDate)[0:4])]\n","print(file_names)"]},{"cell_type":"markdown","metadata":{"id":"W9g-TzRqiC_0"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RcSyBGjQiC_1"},"outputs":[],"source":["loc_file = curr_directory + '/event_' + str(eventDate) + '/GIC/gic_monitors.csv'\n","\n","latitude_file_path = loc_file\n","\n","# latitude_file_path = 'C:\\\\Users\\\\19606\\\\Desktop\\\\GMDandGIC\\\\GICdata\\\\event_20131002\\\\gic_monitors.csv'\n","latitude_df = pd.read_csv(latitude_file_path)\n","latitude_dict = pd.Series(latitude_df[' Latitude'].values, index=latitude_df['Device ID']).to_dict()\n","latitude_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKW0p3Y5iC_2"},"outputs":[],"source":["def determine_max_level(data_length, wavelet='db4'):\n","    return pywt.swt_max_level(data_length)\n","\n","def modwt(data, wavelet, level):\n","    return pywt.swt(data, wavelet, level=level, start_level=0)\n","\n","def wavelet_cross_correlation(wt1, wt2):\n","    corrs = []\n","    for coeff1, coeff2 in zip(wt1, wt2):\n","        corr = np.corrcoef(coeff1[0], coeff2[0])[0, 1]\n","        corrs.append(corr)\n","    return corrs\n","\n","def sliding_window_cross_correlation(wt1, wt2, window_size=30):\n","\n","    assert len(wt1) == len(wt2)\n","    sliding_corrs = []\n","\n","    for start in range(len(wt1) - window_size + 1):\n","        end = start + window_size\n","        window_corr = np.corrcoef(wt1[start:end], wt2[start:end])[0, 1]\n","        sliding_corrs.append(window_corr)\n","\n","    return sliding_corrs\n","\n","def make_length_even(data):\n","    return data if len(data) % 2 == 0 else np.append(data, data[-1])\n","edges = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUFURNuSiC_2"},"outputs":[],"source":["modwt_results = {}\n","\n","for file_name in file_names:\n","    file_path = os.path.join(folder_path, file_name)\n","    df = pd.read_csv(file_path)\n","    df[time_column] = pd.to_datetime(df[time_column])\n","    df = df.drop_duplicates(subset=time_column).set_index(time_column)\n","    df = df.reindex(pd.date_range(start=df.index.min(), end=df.index.max(), freq='T'), method='nearest')\n","    resampled_df = df.resample('T').mean().interpolate(method='spline', order=3)\n","    gic_signal = make_length_even(resampled_df[gic_column].values)\n","    level = determine_max_level(len(gic_signal))\n","    modwt_result = modwt(gic_signal, 'db4', level)\n","    file_name = file_name.split('/')[-1]\n","    modwt_results[file_name] = modwt_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-WYVGECFiC_2"},"outputs":[],"source":["print(\"The wavelet transform coefficients of '2013E02_10056.csv' is:\")\n","print(modwt_results['2013E02_10056.csv'])\n","print(\"The wavelet transform coefficients of '2013E02_10121.csv' is:\")\n","print(modwt_results['2013E02_10121.csv'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJ85KU1ciC_3"},"outputs":[],"source":["desired_length = 1622\n","\n","filtered_modwt_results = {file: modwt_vals for file, modwt_vals in modwt_results.items() if len(modwt_vals[0][0]) == desired_length}\n","\n","filtered_modwt_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FVBRzKDiC_3"},"outputs":[],"source":["sliding_correlations = {}\n","for (file1, wt1), (file2, wt2) in itertools.combinations(filtered_modwt_results.items(), 2):\n","    corrs_over_time = sliding_window_cross_correlation(wt1[0][0], wt2[0][0])\n","    sliding_correlations[(file1, file2)] = corrs_over_time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWRTgxfZiC_3"},"outputs":[],"source":["file_pair_to_visualize = (file_names[0], file_names[9])\n","correlations_over_time = sliding_correlations[file_pair_to_visualize]\n","time_axis = range(len(correlations_over_time))\n","print(time_axis)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8MreMaKiC_3"},"outputs":[],"source":["correlation_threshold = 0.8\n","\n","node_degrees = {file_name: [0] * len(time_axis) for file_name in file_names}\n","\n","for (file1, file2), correlations in sliding_correlations.items():\n","    for t, corr in enumerate(correlations):\n","        if abs(corr) > correlation_threshold:\n","            node_degrees[file1][t] += 1\n","            node_degrees[file2][t] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1IB_36wiC_4"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.patches import Circle\n","from matplotlib.collections import PatchCollection\n","import numpy as np\n","\n","cmap = plt.cm.hot\n","\n","max_correlations = {}\n","for node in node_degrees:\n","    max_correlations[node] = np.full(len(time_axis), -np.inf)\n","\n","for (node1, node2), correlations in sliding_correlations.items():\n","    for i, corr in enumerate(correlations):\n","        max_correlations[node1][i] = max(max_correlations[node1][i], corr)\n","        max_correlations[node2][i] = max(max_correlations[node2][i], corr)\n","max_correlations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aCmIi6s8iC_4"},"outputs":[],"source":["node_degrees = {file_name: [0] * len(time_axis) for file_name in file_names}\n","\n","for (file1, file2), correlations in sliding_correlations.items():\n","    for t, corr in enumerate(correlations):\n","        if abs(corr) > correlation_threshold:\n","            node_degrees[file1][t] += 1\n","            node_degrees[file2][t] += 1\n","node_degrees_1 = {file_name.split('_')[-1].split('.')[0]: degrees\n","                        for file_name, degrees in node_degrees.items()}\n","node_degrees_1 = {int(key): value for key, value in node_degrees_1.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OSH__UFdiC_4"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Circle\n","\n","cmap = plt.cm.viridis\n","norm = plt.Normalize(vmin=0, vmax=1)\n","\n","correlations_info = {node: {'values': [], 'exceeds_80_percentile': False} for node in node_degrees}\n","\n","for (node1, node2), correlations in sliding_correlations.items():\n","    for i, corr in enumerate(correlations):\n","        if corr > 0.85:  # 只考虑超过0.85的相关性\n","            correlations_info[node1]['values'].append(corr)\n","            correlations_info[node2]['values'].append(corr)\n","\n","# Determine which correlation values exceed the 80% threshold\n","for node, info in correlations_info.items():\n","    if info['values']:\n","        sorted_corrs = sorted(info['values'], reverse=True)\n","\n","        percentile_index = int(np.ceil(0.8 * len(sorted_corrs))) - 1\n","        percentile_value = sorted_corrs[percentile_index]\n","\n","        info['exceeds_80_percentile'] = sorted_corrs[0] > percentile_value\n","\n","# draw the circle and set the color\n","fig, ax = plt.subplots(figsize=(14, 6))\n","\n","\n","for node, degrees_over_time in node_degrees.items():\n","    lat = latitude_dict[int(node.split('_')[-1].split('.')[0])]\n","    for time_idx, degree in enumerate(degrees_over_time):\n","        if correlations_info[node]['exceeds_80_percentile']:\n","\n","            color = cmap(norm(correlations_info[node]['values'][0]))\n","        else:\n","            color = 'white'  # If the threshold of 80% is not exceeded, it is set to white\n","        circle = Circle((time_idx, lat), degree * 0.01, color=color, ec='black', lw=0.1)\n","        ax.add_patch(circle)\n","\n","sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n","sm.set_array([])\n","fig.colorbar(sm, ax=ax)\n","ax.set_xlabel('Time Index')\n","ax.set_ylabel('Latitude')\n","ax.set_xlim(0, len(time_axis))\n","ax.set_ylim(min(latitude_dict.values()), max(latitude_dict.values()))\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-B4rDVq9iC_4"},"outputs":[],"source":["correlations_info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uw1rDLG3iC_4"},"outputs":[],"source":["node_degrees = {file_name: [0] * len(time_axis) for file_name in file_names}\n","\n","for (file1, file2), correlations in sliding_correlations.items():\n","    for t, corr in enumerate(correlations):\n","        if abs(corr) > correlation_threshold:\n","            node_degrees[file1][t] += 1\n","            node_degrees[file2][t] += 1\n","node_degrees = {file_name.split('_')[-1].split('.')[0]: degrees\n","                        for file_name, degrees in node_degrees.items()}\n","node_degrees = {int(key): value for key, value in node_degrees.items()}\n","max_correlations = {int(key.split('_')[-1].split('.')[0]): values\n","                             for key, values in max_correlations.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ow53q0oYiC_5"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib.patches import Circle\n","from matplotlib.collections import PatchCollection\n","import numpy as np\n","\n","# Setup the colormap\n","cmap = plt.cm.viridis\n","norm = plt.Normalize(vmin=np.nanmin([np.nanmin(v) for v in max_correlations.values() if np.isfinite(np.nanmin(v))]),\n","                     vmax=np.nanmax([np.nanmax(v) for v in max_correlations.values() if np.isfinite(np.nanmax(v))]))\n","\n","fig, ax = plt.subplots(figsize=(45, 20))\n","\n","# Create a circle for each node at each time point\n","for node, degrees_over_time in node_degrees.items():\n","    lat = latitude_dict[node]\n","    for time_idx, degree in enumerate(degrees_over_time):\n","        max_corr = max_correlations[node][time_idx]\n","        radius = degree * 0.01  # Adjust the factor for circle size as needed\n","        color = cmap(norm(max_corr)) if np.isfinite(max_corr) else 'white'\n","        # Add black edgecolor for better visibility\n","        circle = Circle((time_idx, lat), radius, color=color, ec='black', lw=0.1)  # lw is the linewidth of the edge\n","        ax.add_patch(circle)\n","\n","# Add colorbar\n","sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n","sm.set_array([])\n","fig.colorbar(sm, ax=ax)\n","\n","# Set axis labels and limits\n","ax.set_xlabel('Time')\n","ax.set_ylabel('Latitude')\n","ax.set_xlim(0, len(time_axis))\n","ax.set_ylim(min(latitude_dict.values()), max(latitude_dict.values()))\n","\n","plt.show()\n"]}],"metadata":{"kernelspec":{"display_name":"py37","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0 (default, Jun 28 2018, 08:04:48) [MSC v.1912 64 bit (AMD64)]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"646b00337df63bb190181c010980e5a476a34d7b019542a470ee8cba2b2cf14f"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}