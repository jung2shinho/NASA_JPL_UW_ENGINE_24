{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6z988E/YCtxUEt8wPgAJl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vtc2xVFXOP16","executionInfo":{"status":"ok","timestamp":1708960319114,"user_tz":480,"elapsed":99889,"user":{"displayName":"Isildur","userId":"15393160715742455989"}},"outputId":"e2875fd4-e2d3-46e8-f6ab-a7cfafe77c66"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'risk-resiliency-spwx'...\n","remote: Enumerating objects: 3226, done.\u001b[K\n","remote: Counting objects: 100% (91/91), done.\u001b[K\n","remote: Compressing objects: 100% (46/46), done.\u001b[K\n","remote: Total 3226 (delta 54), reused 75 (delta 45), pack-reused 3135\u001b[K\n","Receiving objects: 100% (3226/3226), 312.64 MiB | 9.00 MiB/s, done.\n","Resolving deltas: 100% (507/507), done.\n","Updating files: 100% (2896/2896), done.\n","Collecting sunpy\n","  Downloading sunpy-5.1.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: astropy!=5.1.0,>=5.0.6 in /usr/local/lib/python3.10/dist-packages (from sunpy) (5.3.4)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from sunpy) (1.25.2)\n","Requirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from sunpy) (23.2)\n","Collecting parfive[ftp]>=2.0.0 (from sunpy)\n","  Downloading parfive-2.0.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: pyerfa>=2.0 in /usr/local/lib/python3.10/dist-packages (from astropy!=5.1.0,>=5.0.6->sunpy) (2.0.1.1)\n","Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.10/dist-packages (from astropy!=5.1.0,>=5.0.6->sunpy) (6.0.1)\n","Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy) (4.66.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from parfive[ftp]>=2.0.0->sunpy) (3.9.3)\n","Collecting aioftp>=0.17.1 (from parfive[ftp]>=2.0.0->sunpy)\n","  Downloading aioftp-0.22.3-py3-none-any.whl (37 kB)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->parfive[ftp]>=2.0.0->sunpy) (4.0.3)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->parfive[ftp]>=2.0.0->sunpy) (3.6)\n","Installing collected packages: aioftp, parfive, sunpy\n","Successfully installed aioftp-0.22.3 parfive-2.0.2 sunpy-5.1.1\n","Collecting zeep\n","  Downloading zeep-4.2.1-py3-none-any.whl (101 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.2/101.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.2.0 in /usr/local/lib/python3.10/dist-packages (from zeep) (23.2.0)\n","Collecting isodate>=0.5.4 (from zeep)\n","  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from zeep) (4.9.4)\n","Requirement already satisfied: platformdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from zeep) (4.2.0)\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from zeep) (2.31.0)\n","Collecting requests-toolbelt>=0.7.1 (from zeep)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests-file>=1.5.1 (from zeep)\n","  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from zeep) (2023.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate>=0.5.4->zeep) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->zeep) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->zeep) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->zeep) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->zeep) (2024.2.2)\n","Installing collected packages: isodate, requests-toolbelt, requests-file, zeep\n","Successfully installed isodate-0.6.1 requests-file-2.0.0 requests-toolbelt-1.0.0 zeep-4.2.1\n","Collecting climextremes\n","  Downloading climextremes-0.3.1-py3-none-any.whl (6.3 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from climextremes) (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from climextremes) (1.5.3)\n","Requirement already satisfied: rpy2 in /usr/local/lib/python3.10/dist-packages (from climextremes) (3.4.2)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from climextremes) (5.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->climextremes) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->climextremes) (2023.4)\n","Requirement already satisfied: cffi>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from rpy2->climextremes) (1.16.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from rpy2->climextremes) (3.1.3)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10.0->rpy2->climextremes) (2.21)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->climextremes) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->rpy2->climextremes) (2.1.5)\n","Installing collected packages: climextremes\n","Successfully installed climextremes-0.3.1\n","Collecting drms\n","  Downloading drms-0.7.1-py3-none-any.whl (36 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from drms) (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from drms) (1.5.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from drms) (23.2)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->drms) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->drms) (2023.4)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->drms) (1.16.0)\n","Installing collected packages: drms\n","Successfully installed drms-0.7.1\n"]}],"source":["!git clone https://github.com/jack-eddy-symposium/risk-resiliency-spwx.git\n","# !du -sh /content/risk-resiliency-spwx/data\n","\n","import numpy as np\n","import pandas as pd\n","import glob\n","import os\n","\n","import datetime\n","from datetime import datetime, timedelta\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","!pip install sunpy\n","!pip install zeep\n","!pip install climextremes\n","!pip install drms\n","\n","from sunpy.net import Fido\n","from sunpy.net import attrs as a\n","from sunpy.timeseries import TimeSeries\n","\n","\n","from scipy.integrate import trapz\n","\n"]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","\n","# Base data directory containing event folders\n","base_data_path = '/content/risk-resiliency-spwx/data'\n","\n","# List to collect max rows from all files in all events\n","max_row_list = []\n","\n","# Loop over every event folder in the data directory\n","for event_folder in os.listdir(base_data_path):\n","    event_path = os.path.join(base_data_path, event_folder)\n","\n","    # Check if it's a directory\n","    if os.path.isdir(event_path):\n","        # Construct path to the GIC subfolder within the event folder\n","        gic_path = os.path.join(event_path, 'GIC')\n","\n","        # Check if the GIC subfolder exists\n","        if os.path.exists(gic_path) and os.path.isdir(gic_path):\n","\n","            csv_files = [os.path.join(gic_path, f) for f in os.listdir(gic_path) if f.endswith('.csv')]\n","            csv_files.sort()  # Sort files by name\n","\n","            # Exclude the last two CSV files\n","            csv_files_to_process = csv_files[:-2]\n","\n","            # Process each CSV file\n","            for single_path in csv_files_to_process:\n","                df = pd.read_csv(single_path)\n","                idx = df['GICMeasured'].idxmax()\n","                max_row = df.loc[idx]\n","                max_row_list.append(max_row)\n","\n","# Convert the list to a DataFrame\n","max_rows_df = pd.DataFrame(max_row_list)\n","max_rows_df\n","\n","\n","# Convert 'sampledatetime' column to datetime format\n","max_rows_df['SampleDateTime'] = pd.to_datetime(max_rows_df['SampleDateTime'])\n","\n","# Extract the year from the 'sampledatetime' column\n","max_rows_df['year'] = max_rows_df['SampleDateTime'].dt.year\n","\n","# Save the modified DataFrame to a new CSV file\n","max_rows_df.to_csv('max_rows_data.csv', index=False)\n","\n","# Load the CSV file into a DataFrame\n","max_rows_df = pd.read_csv('max_rows_data.csv')\n","\n","# Print\n","print(max_rows_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzdRJDl9AWMn","executionInfo":{"status":"ok","timestamp":1708964176679,"user_tz":480,"elapsed":46077,"user":{"displayName":"Isildur","userId":"15393160715742455989"}},"outputId":"083b2292-0559-461b-ab8d-478dfca94e48"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["      GICDeviceID       SampleDateTime  GICMeasured  year\n","0           10063  2017-09-28 03:17:32         0.50  2017\n","1           10066  2017-09-27 18:08:10         1.35  2017\n","2           10067  2017-09-28 20:21:00        -1.13  2017\n","3           10071  2017-09-28 14:04:00         4.29  2017\n","4           10074  2017-09-28 07:00:30         2.08  2017\n","...           ...                  ...          ...   ...\n","1849        10356  2021-05-12 15:00:00         0.42  2021\n","1850        10357  2021-05-12 04:57:30        48.70  2021\n","1851        10359  2021-05-13 04:43:38         1.27  2021\n","1852        10441  2021-05-13 11:15:00        -5.73  2021\n","1853        10442  2021-05-12 02:40:05         3.67  2021\n","\n","[1854 rows x 4 columns]\n"]}]},{"cell_type":"code","source":["# def clean_GIC_data(df,df_sensor_locations,threshold):\n","#     '''\n","#     - delete repeated time points\n","#     - delete station if all time points are empty\n","#     - Find how many of the measurements differ from one time step to the next and delete station if on average measurements differ less than once in 5 minutes\n","#     - remove repeat stations from files\n","#     - remove stations with an unacceptable level of bias\n","\n","#     '''\n","\n","#     # Delete repeated time points\n","#     df = df.drop(df[df.duplicated(subset='datetimes', keep='first')].index)\n","\n","#     cols_to_drop = []\n","#     for id_loop in df.columns.to_list():\n","\n","#         if (id_loop == 'datetimes'):\n","#             continue\n","\n","#         # Delete station if most time points are empty\n","#         if ( (df[id_loop].isna().mean() * 100) > 80.):\n","#             cols_to_drop.append(id_loop)\n","#             continue\n","\n","#     print('cols_to_drop after first filter = {}'.format(cols_to_drop))\n","\n","#     # remove repeat stations from files\n","#     duplicate_lats = df_sensor_locations.duplicated(subset=' Latitude',keep='first')\n","#     duplicate_lons = df_sensor_locations.duplicated(subset=' Longitude',keep='first')\n","#     duplicate_locations = df_sensor_locations['Device ID'].values[np.where( (duplicate_lats.values==True) & (duplicate_lons.values==True) )]\n","\n","#     for d in duplicate_locations:\n","#         try:\n","#             #print(df[str(d)])\n","#             cols_to_drop.append(str(d))\n","#         except:\n","#             print('no df entry for devide ID {}'.format(str(d)))\n","# #         # Check I wrote to guarantee the code above removes duplicate stations (keeping only one of the same lat-long)\n","# #         for a in df_sensor_locations['Device ID'].values[np.where( (duplicate_lats.values==True) & (duplicate_lons.values==True) )]:\n","# #             idx = df_sensor_locations.index[df_sensor_locations['Device ID'] == a].tolist()\n","# #             print('the index of the duplicated lat and lon = {}'.format(a))\n","# #             print(' \\t --> {}'.format( df_sensor_locations[idx[0]-2:idx[0]+3] ))\n","\n","#     print('cols_to_drop after second filter = {}'.format(cols_to_drop))\n","\n","#     # Find how many of the measurements differ from one time step to the next and delete station if measurements repeat for more than 60% of the data\n","#     for device_id in df.columns.to_list():\n","#         if device_id == 'datetimes':\n","#             continue\n","#         count_zeros = np.count_nonzero(np.diff(df[device_id].values) == 0)\n","#         tmp_len = len(np.argwhere(np.isfinite(df[device_id].values)==True))\n","#         tmp = (count_zeros/tmp_len)*100\n","# #             print('percentage of repeated values = {}'.format( tmp  ))\n","#         if tmp > 60.:\n","#             cols_to_drop.append(device_id)\n","\n","#     print('cols_to_drop after third filter = {}'.format(cols_to_drop))\n","\n","\n","\n","#     # Catch bias that is near the threshold level (would cause issues with burst statistics)\n","#     for device_id in df.columns.to_list():\n","#         if device_id == 'datetimes':\n","#             continue\n","#         med_tmp = np.nanmedian(df[device_id].values)\n","#         # apply conservative value for an unacceptable level of bias\n","#         if abs(med_tmp) >= (threshold/2):\n","#             cols_to_drop.append(device_id)\n","\n","#     print('cols_to_drop after fourth filter = {}'.format(cols_to_drop))\n","\n","\n","\n","#     cols_to_drop = list(set(cols_to_drop))\n","#     print('Total number of columns to drop = {}'.format(len(cols_to_drop)))\n","# #     # some stations were removed in the process of creating the integrated data files, so need to check if all columns to remove exist\n","#     cols_to_drop_final = []\n","#     for c in cols_to_drop:\n","#         if any( str(c) in x for x in df.columns.to_list()):\n","#             cols_to_drop_final.append(str(c))\n","#         else:\n","#             print('{} is NOT in the columns list'.format(c))\n","\n","#     df = df.drop(cols_to_drop_final,axis=1)\n","\n","#     return df\n","\n","# def visualize_dataframe_columns(df):\n","#     fig, axs = plt.subplots(len(df.columns), 1, figsize=(6, 1.5*len(df.columns)))\n","\n","#     for i, col in enumerate(df.columns):\n","#         axs[i].plot(df[col])\n","#         axs[i].set_ylabel(col, rotation=0, labelpad=20, fontsize=10)\n","#         axs[i].set_yticklabels([])\n","#         axs[i].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n","\n","#     plt.tight_layout()\n","#     plt.show()\n","\n","# def visualize_dataframe_singlecolumn(df,col_name):\n","#     fig, axs = plt.subplots(1, 1, figsize=(10,8))\n","\n","#     axs.plot(df['datetimes'],df[col_name])\n","#     axs.set_ylabel(col_name, rotation=0, labelpad=20, fontsize=10)\n","# #     axs[i].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n","\n","#     plt.tight_layout()\n","#     plt.show()"],"metadata":{"id":"ldugtZgqQDRO","executionInfo":{"status":"ok","timestamp":1708960833042,"user_tz":480,"elapsed":1,"user":{"displayName":"Isildur","userId":"15393160715742455989"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["POT"],"metadata":{"id":"nP1BA-N-IYPn"}},{"cell_type":"code","source":["import numpy as np\n","import climextremes\n","import pandas as pd\n","\n","max_rows_df = pd.read_csv('max_rows_data.csv')\n","\n","# Define the threshold for extreme precipitation events\n","threshold = 2\n","\n","# Filter out values that exceed the threshold\n","exceedance_values = max_rows_df[max_rows_df['GICMeasured'] > threshold]\n","\n","# Determine the range of years\n","first_year = min(max_rows_df['year'])\n","last_year = max(max_rows_df['year'])\n","\n","# Define the years for predicting return values (next 100 years)\n","years_to_predict = np.arange(last_year + 1, last_year + 101)\n","\n","# Perform extreme value analysis\n","stationary_fit = True\n","\n","if stationary_fit:\n","    result = climextremes.fit_pot(np.array(exceedance_values['GICMeasured']),\n","                                  nBlocks=len(exceedance_values['year']),\n","                                  threshold=threshold,\n","                                  firstBlock=first_year,\n","                                  blockIndex=np.array(exceedance_values['year']),\n","                                  getParams=True,\n","                                  returnPeriod=100,\n","                                  returnValue=0.61,\n","                                  bootSE=False)\n","\n","else:\n","    # Perform extreme value analysis with a non-stationary fit\n","    # may need to provide additional parameters for a non-stationary fit based on the requirements of the climextremes package\n","    pass\n","\n","# Access the return values for the next 100 years\n","return_values_100_years = result['returnValue']\n","print(\"return value is: \", return_values_100_years)\n","print(result['se_returnValue'])       # return value standard error (asymptotic)\n","#print(result['se_returnValue_boot'])  # return value standard error (bootstrapping)\n","\n","\n","# The example code for reference\n","# firstYr = min(Fort.year)\n","# yrs = np.array(range(int(firstYr), int(max(Fort.year)+1)))\n","# nYrs = len(yrs)\n","# yrsToPred = np.array([min(Fort.year), max(Fort.year)])\n","\n","# threshold = 0.395\n","\n","# FortExc = Fort[Fort.Prec > threshold]\n","\n","# # stationary fit\n","# result = climextremes.fit_pot(np.array(FortExc.Prec), nBlocks = nYrs, threshold = threshold, firstBlock = firstYr,\n","#blockIndex = np.array(FortExc.year), getParams = True, returnPeriod = 20, returnValue = 3.5, bootSE = True)\n","# result['returnValue']\n","# result['se_returnValue']       # return value standard error (asymptotic)\n","# result['se_returnValue_boot']  # return value standard error (bootstrapping)\n","# result['logReturnProb']        # log of probability of exceeding 'returnValue'\n","# result['mle']                  # MLE array\n","# result['mle_names']            # names for MLE array\n","# result['mle'][2]               # MLE for shape parameter\n","\n","# result['numBootFailures']      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution\n","\n","# # nonstationary fit with location linear in year and two return values requested\n","# result = climextremes.fit_pot(np.array(FortExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold,\n","#blockIndex = np.array(FortExc.year), locationFun = 1, getParams = True,\n","#returnPeriod = np.array([20, 30]), returnValue = 3.5, xNew = yrsToPred, bootSE = False)\n","# result['returnValue']\n","# result['se_returnValue']\n","\n","# # fit with location a function of two covariates\n","# # here I'll use year and a random vector just to illustrate syntax\n","# # make 'x' be a 2-column numpy array, each column a covariate\n","# # 'xNew' also needs to have 2 columns, each row is a different set of covariate values\n","# tmp = np.random.rand(nYrs)\n","# covByBlock = np.c_[yrs, np.random.rand(nYrs)]\n","# result = climextremes.fit_pot(np.array(FortExc.Prec), x = covByBlock, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold, blockIndex = np.array(FortExc.year), locationFun = np.array([1,2]), getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = np.array([[min(Fort.year), 0], [max(Fort.year), 0]]), bootSE = False)\n","\n","# # with declustering (using max of exceedances on contiguous days)\n","# result = climextremes.fit_pot(np.array(FortExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold, blockIndex = np.array(FortExc.year), index = np.array(FortExc.obs), locationFun = 1, declustering = \"noruns\", getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = yrsToPred, bootSE = False)\n","# result['returnValue']\n","# result['se_returnValue']\n","\n","# # with declustering (consider sequential blocks of 5 days and only use the max of any exceedances within a block)\n","# result = climextremes.fit_pot(np.array(FortExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, threshold = threshold, blockIndex = np.array(FortExc.year), index = np.array(FortExc.obs), locationFun = 1, declustering = 5, getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = yrsToPred, bootSE = False)\n","# result['returnValue']\n","# result['se_returnValue']\n","\n","# # with replicates; for illustration here, I'll just duplicate the Fort data\n","# result = climextremes.fit_pot(np.append(np.array(FortExc.Prec), np.array(FortExc.Prec)), x = yrs, firstBlock = firstYr, nBlocks = nYrs, nReplicates = 2, threshold = threshold, blockIndex = np.append(FortExc.year, FortExc.year), locationFun = 1, getParams = True, returnPeriod = 20, returnValue = 3.5, xNew = yrsToPred, bootSE = False)\n","# result['returnValue']\n","# result['se_returnValue']\n","\n","# # analysis of seasonal total precipitation\n","# tmp = Fort[np.logical_and(Fort['month'] < 9, Fort['month'] > 5)]\n","# FortSummerTotal = tmp.groupby('year').sum()[['Prec']]\n","# FortSummerTotal.reset_index(inplace=True)\n","# threshold = np.percentile(FortSummerTotal.Prec, 80)\n","# FortSummerTotalExc = FortSummerTotal[FortSummerTotal.Prec > threshold]\n","\n","# result = climextremes.fit_pot(np.array(FortSummerTotalExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, blockIndex = np.array(FortSummerTotalExc.year), locationFun = 1, threshold = threshold, getParams = True, returnPeriod = 20, returnValue = 10, xNew = yrsToPred, bootSE = False)\n","# result['returnValue']\n","# result['se_returnValue']\n","\n","# # modifying control arguments and seeing more information on the optimization\n","# result = climextremes.fit_pot(np.array(FortSummerTotalExc.Prec), x = yrs, firstBlock = firstYr, nBlocks = nYrs, blockIndex = np.array(FortSummerTotalExc.year), locationFun = 1, threshold = threshold, getParams = True, returnPeriod = 20, returnValue = 10, xNew = yrsToPred, bootSE = True, bootControl = {'n':150, 'seed':3}, getFit = True)\n","# result['info']   # information on the optimization\n","# result['info']['counts']  # number of evaluations in the optimization\n","# result['info']['counts_names'] # names to interpret 'counts'\n","# result['numBootFailures']      # number of bootstrap datasets for which the model could not be fit; if this is non-negligible relative to the number of bootstrap samples (default of 250), interpret the bootstrap results with caution\n","\n","# # result['fit']  # voluminous output from the R function that does the fitting"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEa1_uCqSsgq","executionInfo":{"status":"ok","timestamp":1708966612379,"user_tz":480,"elapsed":378,"user":{"displayName":"Isildur","userId":"15393160715742455989"}},"outputId":"b58eb33a-ddee-4fac-8d36-58809f2e344a"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["return value is:  [38.00563858]\n","[2.93335284]\n"]}]}]}